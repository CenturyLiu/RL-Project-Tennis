{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem with Batch Normalization layer for action choice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Background knowledge\n",
    "\n",
    "- Batch Normalization\n",
    "\n",
    "Batch normalization is intended to increase stability when training a neural network. Batch normalization normalizes the output of a previous activation layer by subtracting the batch mean and dividing by the batch standard deviation. [source](https://towardsdatascience.com/batch-normalization-in-neural-networks-1ac91516821c)\n",
    "\n",
    "- Actor network for ddpg and maddpg\n",
    "\n",
    "Both ddpg and maddpg includes local actor network for action selection and target actor network for action estimation and update. Both local and target actor network take state(s)/observation(s) as input, and output action based on model parameters. The **local** actor network typically takes **one** state/observation input. The **target** actor network typically takes a **batch** of states as input.\n",
    "\n",
    "#### Problem statement\n",
    "\n",
    "If we add batchnorm layer into actor network, the input **batch** states/observations for the **target actor** network will be **normalized**, while the state/observation for **local actor network** remain the **same** (batchnorm doesn't apply to single input). In other words, we are choosing action based on observation, while estimating action based on a **changed version** of the same state. The **estimated action** (by target actor network) will be largely **different** from the **real action** (by local actor network) even the target actor network is identical to the local actor network. See the example below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example\n",
    "First define an actor model. Here I directly use the actor model for maddpg and uses batchnorm in its forward function (line 47)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def norm_init(layer):\n",
    "    fan_in = layer.weight.data.size()[0]\n",
    "    return 1./np.sqrt(fan_in)\n",
    "\n",
    "def hidden_init(layer):\n",
    "    fan_in = layer.weight.data.size()[0]\n",
    "    lim = 1. / np.sqrt(fan_in)\n",
    "    return (-lim, lim)\n",
    "\n",
    "class Actor(nn.Module):\n",
    "    \"\"\"Actor (Policy) Model.\"\"\"\n",
    "\n",
    "    def __init__(self, state_size, action_size, seed, fc1_units = 256, fc2_units = 128):\n",
    "        \"\"\"Initialize parameters and build model.\n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): Dimension of each state\n",
    "            action_size (int): Dimension of each action\n",
    "            seed (int): Random seed\n",
    "            fc1_units (int): Number of nodes in first hidden layer\n",
    "            fc2_units (int): Number of nodes in second hidden layer\n",
    "        \"\"\"\n",
    "        super(Actor, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        self.fc1 = nn.Linear(state_size, fc1_units)\n",
    "        self.fc2 = nn.Linear(fc1_units,fc2_units)\n",
    "        self.fc3 = nn.Linear(fc2_units, action_size)\n",
    "        self.batchnorm_1 = nn.BatchNorm1d(fc1_units)\n",
    "        \n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.fc1.weight.data.uniform_(*hidden_init(self.fc1))\n",
    "        self.fc2.weight.data.uniform_(*hidden_init(self.fc2))\n",
    "        self.fc3.weight.data.uniform_(-3e-3, 3e-3)\n",
    "\n",
    "    def forward(self, state):\n",
    "        \"\"\"Build an actor (policy) network that maps states -> actions.\"\"\"\n",
    "        #x = F.relu(self.batchnorm_1(self.fc1(state)))\n",
    "        if state.dim() != 1:\n",
    "            x = F.relu(self.batchnorm_1(self.fc1(state)))\n",
    "            #x = F.relu(self.fc1(state))\n",
    "        else:\n",
    "            x = F.relu(self.fc1(state))\n",
    "        \n",
    "        \n",
    "        x = F.relu(self.fc2(x))\n",
    "        return F.tanh(self.fc3(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then instantiate an actor and prepare random states as input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "actor = Actor(24,2,4)\n",
    "single_state = torch.rand(24)\n",
    "batch_states = torch.stack([single_state,torch.rand(24),torch.randn(24)]) # put the single_state along with\n",
    "                                                                          # other random states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pass states into the actor, get actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "single_action:  tensor([-0.0143,  0.0236], grad_fn=<TanhBackward>)\n",
      "batch_action:  tensor([[-0.0164,  0.0191],\n",
      "        [-0.0141,  0.0240],\n",
      "        [-0.0198,  0.0162]], grad_fn=<TanhBackward>)\n",
      "batch_action[0]:  tensor([-0.0164,  0.0191], grad_fn=<SelectBackward>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shijiliu/anaconda3/envs/drlnd/lib/python3.6/site-packages/torch/nn/functional.py:1558: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    }
   ],
   "source": [
    "single_action = actor(single_state)\n",
    "batch_action = actor(batch_states)\n",
    "print(\"single_action: \", single_action)\n",
    "print(\"batch_action: \", batch_action)\n",
    "print(\"batch_action[0]: \", batch_action[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that batch_action[0] is different from the single_action due to the normalization step we take. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the target actor network is used to estimate action for next states, and then put into the target actor network to derive the TD error term, the estimated TD error will be inaccurate. This may lead to the agents' not able to solve the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlnd",
   "language": "python",
   "name": "drlnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
